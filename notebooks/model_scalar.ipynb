{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2714fa7b-bfc4-4bcd-979a-bf7c15707a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools as it\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "import olmo\n",
    "\n",
    "os.environ[\"SCRATCH_DIR\"] = \"no_exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a03f529-6fcc-4b56-a975-ee8ea3cb4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_config(\n",
    "    config,\n",
    "    flops_ratio,\n",
    "    axis_divisor=128,\n",
    "    ceil=False,\n",
    "    mode=\"inference-flops\",\n",
    "    other_updates={},\n",
    "):\n",
    "    assert flops_ratio\n",
    "    config = deepcopy(config)\n",
    "    config.init_device = \"meta\"\n",
    "    head_dim = config.d_model // config.n_heads\n",
    "    if not config.mlp_hidden_size:\n",
    "        config.mlp_hidden_size = config.d_model * config.mlp_ratio\n",
    "\n",
    "    # estimate the flops of a config\n",
    "    def f(C):\n",
    "        C.init_device = \"meta\"\n",
    "        model = olmo.model.OLMo(C)\n",
    "        if mode == \"inference-flops\":\n",
    "            return model.num_fwd_flops\n",
    "        elif mode == \"params\":\n",
    "            return model.num_params()\n",
    "        elif mode == \"params-non-embedding\":\n",
    "            return model.num_params(include_embedding=False)\n",
    "        elif mode == \"train-flops\":\n",
    "            return model.num_fwd_flops + model.num_bck_flops\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown mode {mode}\")\n",
    "\n",
    "    def make_config(d_model, n_layers, mlp_hidden_size, do_updates=True):\n",
    "        C = deepcopy(config)\n",
    "        C.d_model = d_model\n",
    "        C.n_heads = C.d_model // head_dim\n",
    "        C.mlp_hidden_size = mlp_hidden_size\n",
    "        C.n_layers = n_layers\n",
    "\n",
    "        if do_updates:\n",
    "            for key, val in other_updates.items():\n",
    "                setattr(C, key, val)\n",
    "        return C\n",
    "\n",
    "    # reparameterize so only valid configs are reachable\n",
    "    def param(d, n, h):\n",
    "        return (\n",
    "            config.d_model + head_dim * d,\n",
    "            config.n_layers + n,\n",
    "            config.mlp_hidden_size + axis_divisor * h,\n",
    "        )\n",
    "\n",
    "    def r(d, n, h):\n",
    "        ratios = np.array(\n",
    "            [\n",
    "                1 + head_dim * d / config.d_model,\n",
    "                1 + n / config.n_layers,\n",
    "                1 + axis_divisor * h / config.mlp_hidden_size,\n",
    "            ]\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return np.log(ratios)\n",
    "\n",
    "    def g(d, n, h, do_updates=True):\n",
    "        return f(make_config(*param(d, n, h), do_updates=do_updates))\n",
    "\n",
    "    base_flops = g(0, 0, 0, do_updates=False)\n",
    "    target_flops = base_flops * flops_ratio\n",
    "\n",
    "    # fit a polynomial to g\n",
    "    Q = np.array(list(it.product(*[[0, 1, 2]] * 3)))\n",
    "    one = np.ones_like(Q[:, 0])\n",
    "    QQ = np.vstack([Q[:, 0] ** a * Q[:, 1] ** b * Q[:, 2] ** c for a, b, c in Q]).T\n",
    "    Qg = np.array([g(*row) / base_flops for row in Q])\n",
    "    coeff = np.linalg.lstsq(QQ, Qg, rcond=None)[0]\n",
    "\n",
    "    def g2(d, n, h):\n",
    "        return (\n",
    "            np.array([d**a * n**b * h**c for a, b, c in Q]).dot(coeff)\n",
    "            * base_flops\n",
    "        )\n",
    "\n",
    "    # double check the predictions are matching\n",
    "    assert round(g2(3, 4, 5)) == g(3, 4, 5)\n",
    "    assert round(g2(5, 4, 6)) == g(5, 4, 6)\n",
    "    assert round(g2(2, 7, 3)) == g(2, 7, 3)\n",
    "\n",
    "    # given d and n, solve for h\n",
    "    def solve_h(d, n):\n",
    "        f0, f1 = g2(d, n, 0), g2(d, n, 1)\n",
    "        slope = f1 - f0\n",
    "        rounder = np.ceil if ceil else np.floor\n",
    "        return int(rounder((target_flops - f0) / slope))\n",
    "\n",
    "    # enumerate all viable d and n\n",
    "    best, best_l = None, float(\"inf\")\n",
    "    \n",
    "    d2 = 0\n",
    "    while True:\n",
    "        if g2(d2, 0, 0) > target_flops:\n",
    "            break\n",
    "\n",
    "        n2 = 0\n",
    "        while True:\n",
    "            if g2(d2, n2, 0) > target_flops:\n",
    "                break\n",
    "\n",
    "            h2 = solve_h(d2, n2)\n",
    "            r2 = r(d2, n2, h2)\n",
    "            l2 = r2.std()\n",
    "            if l2 < best_l:\n",
    "                best_l, best = l2, (d2, n2, h2)\n",
    "\n",
    "            n2 += 1\n",
    "        d2 += 1\n",
    "        \n",
    "    d2 = 0\n",
    "    while True:\n",
    "        if g2(d2 - 1, 0, 0) < target_flops:\n",
    "            break\n",
    "\n",
    "        n2 = 0\n",
    "        while True:\n",
    "            if g2(d2, n2 - 1, 0) < target_flops:\n",
    "                break\n",
    "                \n",
    "            h2 = solve_h(d2, n2)\n",
    "            r2 = r(d2, n2, h2)\n",
    "            if not np.isinf(r2).any():\n",
    "                l2 = r2.std()\n",
    "                if l2 < best_l:\n",
    "                    best_l, best = l2, (d2, n2, h2)\n",
    "\n",
    "            n2 -= 1\n",
    "        d2 -= 1\n",
    "\n",
    "    opt_d, opt_n, opt_hsize = param(*best)\n",
    "    ratios = tuple(r(*best).tolist())\n",
    "    rel_flops = (g(*best) - target_flops) / target_flops\n",
    "    return (\n",
    "        (opt_d, opt_d // head_dim, opt_n, opt_hsize),\n",
    "        ratios,\n",
    "        rel_flops,\n",
    "        make_config(*param(*best)),\n",
    "    )\n",
    "\n",
    "# scale_config(\n",
    "#     BASE_CONFIG.model, 0.7, mode=\"train-flops\", other_updates=dict(max_sequence_length=1376)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7786f35-5c11-453a-8275-d95c99dd599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_per_step(config):\n",
    "    return config.global_train_batch_size * config.model.max_sequence_length\n",
    "\n",
    "def test_flops_per_step(config):\n",
    "    model = olmo.model.OLMo(config.model)\n",
    "    return tokens_per_step(config) * model.num_fwd_flops\n",
    "    \n",
    "def train_flops_per_step(config):\n",
    "    model = olmo.model.OLMo(config.model)\n",
    "    return tokens_per_step(config) * (model.num_fwd_flops + model.num_bck_flops)\n",
    "    \n",
    "def bytes_per_step(config, encoding_efficiency):\n",
    "    result = config.global_train_batch_size * (\n",
    "        config.model.max_sequence_length\n",
    "        * encoding_efficiency\n",
    "    )\n",
    "    return float(result)\n",
    "    \n",
    "def num_fwd_flops(model):\n",
    "    # embedding table is just a lookup in the forward pass\n",
    "    n_params = model.num_params(include_embedding=False)\n",
    "    # the number of parameters is approximately the number of multiply-accumulates (MAC) in the network\n",
    "    # each MAC has 2 FLOPs - we multiply by 2 ie 2 * n_param\n",
    "    # this gets us FLOPs / token\n",
    "    params_flops_per_token = 2 * n_params\n",
    "    # there are 2 FLOPS per mac; there is A=Q*K^T and out=A*V ops (ie mult by 2)\n",
    "    attn_flops_per_token = (\n",
    "        model.config.n_layers * 2 * 2 * (model.config.d_model * model.config.max_sequence_length)\n",
    "    )\n",
    "    return params_flops_per_token, attn_flops_per_token\n",
    "\n",
    "def num_bck_flops(model):\n",
    "    n_params = model.num_params()\n",
    "    params_flops_per_token = 4 * n_params\n",
    "    attn_flops_per_token = model.config.n_layers * 8 * (model.config.d_model * model.config.max_sequence_length)\n",
    "    return params_flops_per_token, attn_flops_per_token\n",
    "        \n",
    "def model_num_params(config):\n",
    "    model = olmo.model.OLMo(config.model)\n",
    "    return model.num_params()\n",
    "\n",
    "def max_steps(config):\n",
    "    if isinstance(config.max_duration, int):\n",
    "        return config.max_duration\n",
    "    elif isinstance(config.max_duration, str):\n",
    "        if config.max_duration.endswith(\"T\"):\n",
    "            # convert to float *first* to handle scientific notation\n",
    "            max_tokens = int(float(config.max_duration[:-1].strip()))\n",
    "            return math.ceil(max_tokens / (config.global_train_batch_size * config.model.max_sequence_length))\n",
    "        elif config.max_duration.endswith(\"ep\"):\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            # convert to float *first* to handle scientific notation\n",
    "            return int(float(config.max_duration))\n",
    "    else:\n",
    "        raise TypeError(f\"expected int or str for 'max_duration', found {type(config.max_duration)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c1dd33e-a9fc-4bd8-bdf6-e5a6c79d4203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config:\n",
      "TARGET_CONFIG.model.d_model=1280\n",
      "TARGET_CONFIG.model.n_heads=20\n",
      "TARGET_CONFIG.model.n_layers=19\n",
      "TARGET_CONFIG.model.mlp_hidden_size=10240\n",
      "TARGET_CONFIG.model.weight_tying=False\n",
      "TARGET_CONFIG.model.max_sequence_length=2048\n",
      "TARGET_CONFIG.model.vocab_size=200005\n",
      "TARGET_CONFIG.model.embedding_size=200064\n",
      "TARGET_CONFIG.max_duration=10572\n",
      "\n",
      "Model ratios: (0.22314355131420974, 0.17185025692665923, 0.22314355131420974)\n",
      "Tokens: 22,171,090,944\n",
      "Params: 1,010,336,000\n",
      "T/P ratio: 21.9443 (0.99747x Chinchilla)\n"
     ]
    }
   ],
   "source": [
    "# Define the \"base\" config we are going to scale\n",
    "BASE_CONFIG = olmo.config.TrainConfig.load(\n",
    "    \"pretokenization-scripts/nairr/configs/OM2-300M-generic200k.yaml\"\n",
    "    # \"pretokenization-scripts/nairr/configs/OM2-1B-pt200k.yaml\"\n",
    "    # \"../OLMo-alisa/configs/alisa/OLMo2-7B-generic200k.yaml\"\n",
    "    # \"configs/official-1124/OLMo2-7B-stage1.yaml\"\n",
    ")\n",
    "# BASE_CONFIG.max_duration = 76543\n",
    "BASE_TRAIN_FLOPS = max_steps(BASE_CONFIG) * train_flops_per_step(BASE_CONFIG)\n",
    "BASE_ENCODING_EFFICIENCY = 4.458679110036542\n",
    "\n",
    "# Specify the desired scaling parameters\n",
    "TOKENIZER_VOCAB_SIZE = BASE_CONFIG.model.vocab_size\n",
    "TOKENIZER_ENCODING_EFFICIENCY = 6.0887434010717465\n",
    "TOKENIZER_ENCODING_EFFICIENCY = BASE_ENCODING_EFFICIENCY\n",
    "# TOKENIZER_ENCODING_EFFICIENCY = 6.6421079664426035\n",
    "\n",
    "TARGET_TRAIN_FLOPS = BASE_TRAIN_FLOPS \n",
    "# TARGET_MODEL_SCALE = TOKENIZER_ENCODING_EFFICIENCY/BASE_ENCODING_EFFICIENCY\n",
    "TARGET_MODEL_SCALE = 1.58\n",
    "\n",
    "\n",
    "# Do the calculations\n",
    "TARGET_CONFIG = deepcopy(BASE_CONFIG)\n",
    "TARGET_CONFIG.model.vocab_size = TOKENIZER_VOCAB_SIZE\n",
    "TARGET_CONFIG.model.embedding_size = (\n",
    "    TOKENIZER_VOCAB_SIZE + (-TOKENIZER_VOCAB_SIZE) % 128\n",
    ")\n",
    "TARGET_CONFIG.model.max_sequence_length = int(\n",
    "    np.ceil(\n",
    "        BASE_CONFIG.model.max_sequence_length\n",
    "        * BASE_ENCODING_EFFICIENCY\n",
    "        / TOKENIZER_ENCODING_EFFICIENCY\n",
    "    )\n",
    ")\n",
    "\n",
    "if TARGET_MODEL_SCALE:\n",
    "    params, scales, error, new_model_config = scale_config(\n",
    "        BASE_CONFIG.model,\n",
    "        TARGET_MODEL_SCALE,\n",
    "        mode=\"inference-flops\",\n",
    "        other_updates=dict(max_sequence_length=TARGET_CONFIG.model.max_sequence_length),\n",
    "    )\n",
    "    TARGET_CONFIG.model = new_model_config\n",
    "\n",
    "TARGET_CONFIG.max_duration = int(\n",
    "    np.floor(TARGET_TRAIN_FLOPS / train_flops_per_step(TARGET_CONFIG))\n",
    ")\n",
    "TARGET_NUM_PARAMS = model_num_params(TARGET_CONFIG)\n",
    "TARGET_TOKENS = TARGET_CONFIG.max_duration * tokens_per_step(TARGET_CONFIG)\n",
    "TARGET_TOKEN_PARAM_RATIO = TARGET_TOKENS / TARGET_NUM_PARAMS\n",
    "TARGET_TOTAL_BYTES = TARGET_CONFIG.max_duration * bytes_per_step(TARGET_CONFIG, TOKENIZER_ENCODING_EFFICIENCY)\n",
    "# hardcoded for OLMo Mix 2\n",
    "if TARGET_TOTAL_BYTES > 1748032475185 * 0.99:\n",
    "    print(\"Warning: subset does not have enough training bytes!\")\n",
    "print(\"Model config:\")\n",
    "print(f\"{TARGET_CONFIG.model.d_model=}\")\n",
    "print(f\"{TARGET_CONFIG.model.n_heads=}\")\n",
    "print(f\"{TARGET_CONFIG.model.n_layers=}\")\n",
    "if TARGET_CONFIG.model.mlp_hidden_size:\n",
    "    print(f\"{TARGET_CONFIG.model.mlp_hidden_size=}\")\n",
    "else:\n",
    "    print(f\"{TARGET_CONFIG.model.mlp_ratio=}\")\n",
    "\n",
    "if not TARGET_MODEL_SCALE:\n",
    "    print(\"[The above should be unchanged from the baseline.]\")\n",
    "print(f\"{TARGET_CONFIG.model.weight_tying=}\")\n",
    "print(f\"{TARGET_CONFIG.model.max_sequence_length=}\")\n",
    "print(f\"{TARGET_CONFIG.model.vocab_size=}\")\n",
    "print(f\"{TARGET_CONFIG.model.embedding_size=}\")\n",
    "print(f\"{TARGET_CONFIG.max_duration=}\")\n",
    "print()\n",
    "if TARGET_MODEL_SCALE:\n",
    "    print(f\"Model ratios: {scales}\")\n",
    "print(f\"Tokens: {TARGET_TOKENS:,}\")\n",
    "print(f\"Params: {TARGET_NUM_PARAMS:,}\")\n",
    "print(\n",
    "    f\"T/P ratio: {TARGET_TOKEN_PARAM_RATIO:.06} ({TARGET_TOKEN_PARAM_RATIO/22:.05}x Chinchilla)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
